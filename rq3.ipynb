{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RQ3: Are there additional metrics that would make sense?\n",
    "\n",
    "Research question 3 deals with the question, whether or not it makes sense to extend the original metrics in order to track the prediction accuracy on additional code structures such as the following:\n",
    "\n",
    "- Strings\n",
    "- Attributes\n",
    "- Conditions\n",
    "- Comparisons\n",
    "- Tuples\n",
    "\n",
    "These are very common in the source code dataset and therefore could improve the meaningfulness of model evaluation.\n",
    "\n",
    "## Capturing IDs of new metrics\n",
    "\n",
    "The first location where new metrics have to be captured is in the `generate_ast_ids` script ([link](https://github.com/derochs/code-prediction-transformer/blob/master/models/trav_trans/generate_ast_ids.py)).\n",
    "\n",
    "Line [#44](https://github.com/derochs/code-prediction-transformer/blob/master/models/trav_trans/generate_ast_ids.py#L44) introduces a new value metric for Strings. This is done by checking AST nodes for the keyword `Str` and storing the respective node IDs.\n",
    "\n",
    "Line [#58](https://github.com/derochs/code-prediction-transformer/blob/master/models/trav_trans/generate_ast_ids.py#L58) introduces new type metrics, namely `Attributes`, `Conditions`, `Comparisons` and `Tuples`. Analogously to the new value metrics, these new metrics are tracked by storing the node ID for each respective category.\n",
    "\n",
    "## Evaluating new metrics\n",
    "\n",
    "In the `evaluate` script ([link](https://github.com/derochs/code-prediction-transformer/blob/master/evaluate.py)) these new metrics are evaluated by default and for each model that uses this evaluation script. This allows for a comparison of each model and their performance on the new metrics.\n",
    "\n",
    "Line [#72](https://github.com/derochs/code-prediction-transformer/blob/master/evaluate.py#L72) and [#83+](https://github.com/derochs/code-prediction-transformer/blob/master/evaluate.py#L83) initialize lists for each metric that will be filled with their respective mean reciprocal rank scores during evaluation.\n",
    "\n",
    "## Results\n",
    "\n",
    "The following two diagrams show the performance of the added metrics in comparisons with the original metric set by `trav_trans`. The extended leaf node predictions contain the scores for all value metrics as well as the new one (Strings). The extended internal node predictions contain scores for all type metrics, including the newly introduces ones (attributes, conditions, comparisons, tuples). \n",
    "\n",
    "![New Value Metrics](https://raw.githubusercontent.com/derochs/code-prediction-transformer/master/assets/rq3_values.png)\n",
    "![New Type Metrics](https://raw.githubusercontent.com/derochs/code-prediction-transformer/master/assets/rq3_types.png)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
